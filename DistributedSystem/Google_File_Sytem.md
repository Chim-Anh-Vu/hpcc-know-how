# The Gooogle File System

<!-- TOC -->

- [The Gooogle File System](#the-gooogle-file-system)
    - [Tóm tắt](#tóm-tắt)
    - [1. Giới thiệu](#1-giới-thiệu)
    - [2. Góc nhìn cơ bản về thiết kế của GFS](#2-góc-nhìn-cơ-bản-về-thiết-kế-của-gfs)
        - [2.1 Các giả thiết được sử dụng làm cơ sở để xây dựng GFS](#21-các-giả-thiết-được-sử-dụng-làm-cơ-sở-để-xây-dựng-gfs)
        - [2.2 Phương thức tương tác của các application với GFS](#22-phương-thức-tương-tác-của-các-application-với-gfs)
        - [2.3 Kiến trúc của GFS](#23-kiến-trúc-của-gfs)
        - [2.4 Master Node](#24-master-node)
        - [2.5 Chunk size](#25-chunk-size)
        - [2.6 Metadata](#26-metadata)
            - [2.6.1 Cấu trúc của các thông tin lưu trữ trong RAM của master node](#261-cấu-trúc-của-các-thông-tin-lưu-trữ-trong-ram-của-master-node)
            - [2.6.2 Vị trí của chunk](#262-vị-trí-của-chunk)
            - [2.6.3 **Operation log**](#263-operation-log)
        - [2.7 Mô hình consistency của GFS](#27-mô-hình-consistency-của-gfs)
            - [2.7.1 Cách thức đảm bảo sự tin cậy của GFS](#271-cách-thức-đảm-bảo-sự-tin-cậy-của-gfs)

<!-- /TOC -->

## Tóm tắt

Google File System (GFS) là một hệ thống file lưu trữ phân tán được thiết kế và xây dựng để phục vụ một số lượng lớn các ứng dụng phân tán có cường độ trao đổi dữ liệu cao. GFS có tính chịu lỗi - fault tolerance ngay cả khi nó được triển khai trên môi trường phần cứng phổ thông, không chuyên dụnng, đồng thời, Hệ thống GFS cung cấp một hiệu năng tổng thể cao cho cho một lượng lớn các ứng dụng đóng vai trò client, sử dụng GFS để lưu trữ dữ liệu.

GFS được thiết kế với các mục đích giống với các hệ thống File System đã có trước đó, tuy nhiên, sự khác biệt của GFS so với các hệ thống cũ là ở chỗ, GFS được thiết kế dựa trên các đặc điểm hoạt động - trao đổi dữ liệu của các ứng dụng trong các hệ thống khác của Google mà có sử dụng - trao đổi dữ liệu với hệ thống Distributed File System. Bên cạnh đó, cơ sở thiết kế GFS cũng được dựa trên các đặc điểm của môi trường  công nghệ sẽ triển khai GFS của Google. Điều này có nghĩa rằng, GFS được thiết kế để phù hợp với môi trường hoạt động của Google, và được thiết kế nhằm phục vụ tốt nhất cho các ứng dụng của Google, chứ không phải nhằm hướng tới phục vụ cho mọi loại ứng dụng.

Sau khi được thiết kế, GFS đã được triển khai trên một quy mô lớn, phục vụ cho việc lưu trữ và xử lý dữ liệu của các Service trong hệ thống của Google. Bên cạnh đó, GFS cũng phục vụ cho các công việc nghiên cứu và phát triển các công nghệ có liên quan tới dữ liệu lớn. Cụm GFS cluster lớn nhất (tính tới thời điểm xuất bản bài báo) có kích thước hàng trăm terabyte, phân tán trên hàng ngàn ổ đĩa của hàng ngàn máy tính vật lý, và GFS cluster này có khả năng cùng một lúc cung cấp dịch vụ lưu trữ cho hàng trăm client, đáp ứng nhu cầu lưu trữu và xử lý dữ liệu của Google.

Bài viết này sẽ trình bày và thảo luận các khía cạnh khác nhau trong thiết kế của GFS, các tính năng mở rộng của GFS được thiết kế để phục vụ cho các ứng dụng phân tán. Sau đó, bài viết sẽ trình bày báo cáo về các đánh giá hiệu năng trên quy mô thử nghiệm - micro-benchmarks cũng như hiệu năng trên môi trường sử dụng thực tế của GFS.

## 1. Giới thiệu

GFS được thiết kế và xây dựng để phục vụ nhu cầu lưu trữ và xử lý dữ liệu ngày càng gia tăng nhanh chóng của Google. GFS được thiết kế với các mục tiêu giống như các hệ thống Disttributed File System trước đó, các mục tiêu đó là hiệu năng, khả năng scaling, tính tin cậy, và tính sẵn sàng. Tuy nhiên, GFS được thiết kế để phục vụ cho Google, do đó GFS được thiết kế dựa trên cơ sở là các đặc điểm làm việc của các ứng dụng trong hệ thống của Google, cũng như dựa trên các đặc điểm môi trường công nghệ hiện có của Google. Tiếp theo, chúng ta sẽ trình bày về các đặc điểm này, để từ đó cho thấy sự khác biệt về cơ sở phát triển hệ thống đã dẫn tới những sự khác biệt nào giữa thiết kế của GFS so với các hệ thống DFS trước đó.

Đầu tiên, thiết kế của GFS được dựa trên đặc điểm bất cứ thành phần nào ( kể cả phần cứng lẫn phần mềm) trong hệ thống GFS có thể xảy ra hỏng hóc, và sự hỏng hóc của các thành phần trong hệ thống được coi là một sự kiện thường xuyên xảy ra, chứ không phải là một sự cố đặc biệt. GFS được thiết kế để triển khai trên môi trường phần cứng bao gồm hàng nghìn máy tính vật lý đóng vai trò máy lưu trữ. Các máy tính này có cấu hình phần cứng phổ thông, do đó chúng ta khồng đảm bảo về chất lượng của phần cứng các máy tính này. Điều này dẫn tới việc một số phần cứng, phần mềm trên hệ thống có thể xảy ra sự cố và hỏng hóc. Và khi sự cố xảy ra, những dữ liệu lưu trữ trên các máy tính bị hỏng phần cứng sẽ có khả năng bị mất mát, không thể phục hồi được. Các lỗi phần cứng, phần mềm trên hệ thống có thể do nhiều nguồn khác nhau gây ra: Bug của phần mềm, hệ điều hành gặp sự cố, lỗi do con người gây ra, lỗi ổ đĩa cứng, lỗi RAM, lỗi mạng và lỗi do hệ thống điện. Đặc điểm đầu tiên của hệ thống GFS, chính là tất cả các thành phần trên hệ thống, cả phần cứng lẫn phần mềm,đều có thể xảy ra lỗi bất kỳ lúc nào. Để giải quyết vấn đề các thành phần trên hệ thống gặp sự cố, hệ thống GFS phải xây dựng và tích hợp các cơ chế kiểm soát, theo dõi toàn bộ các thành phần trong hệ thống, cơ chế phát hiện lỗi, cơ chế chống lỗi, cũng như cơ chế phục hồi các thành phần bị lỗi khi sự cố xảy ra.

Thứ hai, đa số các file được lưu trữ và xử lý trong hệ thống GFS có đặc điểm là có kích thước rất lớn, lên tới kích cỡ hàng GB. Mỗi một file có nội dung bao gồm rất nhiều các object của các ứng dụng, ví dụ như một file dữ liệu có thể chứa tới hàng nghìn các trang web. Lý do để gom hàng nghìn object vào một file như vậy, là vì khi làm việc với hàng tỉ object có kích thước hàng nghìn TB như vậy, việc lưu trữ mỗi một object vào một file làm giảm hiệu năng của hệ thống, vì quá trình quản lý cũng như băng thông I/O của hệ thống có hàng tỉ file có kích thước nhỏ là chậm và không hiệu quả bằng một hệ thống có các file có kích thước lớn và số lượng file nhỏ.

Thứ ba, đa số các file trong hệ thống được thay đổi bằng cách chèn thêm dữ liệu (append) thay vì ghi đè (overwriting) dữ liệu cũ. Việc thay đổi file bằng cách ghi ngẫu nhiên là hầu như không xảy ra. Đồng thời, hoạt động chủ yếu của hệ thống khi tương tác với các file là hoạt động đọc file, và hầu hết cac hoạt động đọc file trong hệ thống xảy ra tuần tự - sequentially read. Các file trong hệ thống của Google có các đặc điểm trên, là do nhiều ứng dụng xử lý dữ liệu của Google đọc dữ liệu bằng cách quét tuần tự qua một file. Đồng thời, rất nhiều file trong hệ thống của Google được tạo ra bằng cách streamming dữ liệu, ví dụ như truyền video, hay crawl web,... Một số lượng lớn dữ liệu tồn tại dưới dạng file nén, một số file là kết quả của việc nhiều máy tính cùng ghi dữ liệu vào file đó một lúc, hoặc được ghi vào trong nhiều thời điểm khác nhau. Các đặc điểm về phương thức tương tác với các file của các ứng dụng trong hệ thống Google dẫn tới việc trong vấn đề tương tác với các file trong GFS của các ứng dụng, chúng ta cần tối ưu hiệu năng cũng như đảm bảo tính đúng đắn của thao tác nạp thêm dữ liệu vào file (data appending).

Thứ tư, đồng thời với việc GFS được thiết kế để phục vụ tốt nhất cho các ứng dụng của Google, thì các ứng dụng của Google cũng được thiết kế các API riêng biệt để hoạt động tốt với GFS. Bên cạnh đó, mô hình nhất quán dữ liệu của GFS được thiết kế để đơn giản hóa tối đa các tương tác giữa ứng dụng với GFS. Điều này có nghĩa là vai trò đảm bảo tính nhất quán của dữ liệu được đặt vào GFS chứ không phải là nhiệm vụ của các ứng dụng sử dụng GFS. Bên cạnh đó, GFS cũng được thiết kế để phục vụ tốt thao tác nạp thêm dữ liệu, GFS cho phép nhiều ứng dụng cùng 1 lúc nạp thêm dữ liệu vào một file mà không cần phải thực hiện thao tác đồng bộ hóa giữa các ứng dụng này. Thao tác nạp thêm dữ liệu trong GFS sẽ còn được thảo luận thêm trong phần sau của bài viết.

Hàng loạt GFS cluster đã được triển khai để phục vụ các mục đích khác nhau của Google. GFS cluster lớn nhất có hơn 1000 node, chứa 300 TB dữ liệu và phục vụ cho hàng trăm ứng dụng client cùng một lúc.

## 2. Góc nhìn cơ bản về thiết kế của GFS

### 2.1 Các giả thiết được sử dụng làm cơ sở để xây dựng GFS

Chúng ta sẽ đưa ra một số giả thiết làm điều kiện tiền đề cho việc thiết kế hệ thống GFS. Các giả thiết này được đặt ra dựa trên những đặc điểm về ứng dụng sử dụng cũng như các đặc điểm về hệ thống triển khai GFS mà chúng ta đã nêu ra ở phần giới thiệu, và sự có mặt của các giả thiết này vừa đặt ra những thách thức, vừa đem tới các yếu tố thuận lợi cho việc thiết kế GFS. Các giả thiết được đưa ra ở đây là:

- GFS được triển khai trên môi trường hệ thống được xây dựng từ rất nhiều thành phần phần cứng phổ thông, các thành phần trong hệ thống có thể thường xuyên xảy ra lỗi. GFS phải có khả năng thao dõi và phát hiện ra các thành phần nào trong hệ thống đang gặp lỗi, đồng thời GFS vẫn phải có khả năng hoạt động bình thường ngay cả khi một số hoạt động đang gặp lỗi, và sau khi phát hiển ra các thành phần lỗi GFS phải nhanh chóng phục hồi hệ thống để phục hồi các dữ liệu đã bị mất trên các thành phần này.
- Phần lớn các file được lưu trữ và xử lý bởi GFS là các file có kích thước lớn. Số lượng file được lưu trữ trong hệ thống là khoảng vài triệu file, với kích thước của một file thường lớn hơn 100 MB. GFS cần có khả năng quản lý và xử lý hiệu quả các file có kích cỡ GB, do số lượng các file có kích cỡ này trong hệ thống là không hề nhỏ. GFS vẫn cho phép các client application làm việc với các file có kích thước nhỏ, tuy nhiên hệ thống sẽ không tối ưu cho trường hợp này.
- Khối lượng tải mà các Client đặt lên GFS thuộc chủ yếu 2 thao tác sau: Streaming read - đọc theo luồng và random read - đọc ngẫu nhiên. Thao tác streaming read được sử dụng để đọc một lượng nhỏ dữ liệu một lần (vài trăm KB - 1MB), nhưng số lần đọc là rất lớn, và dữ liệu ở các lần đọc liên tiếp nhau cũng nằm lên tiếp nhau ( do vậy thao tác đọc này được gọi là streaming). Thao tác random read thường đọc một lượng nhỏ dữ liệu ở một vị trí nào đó trong file. Các ứng dụng yêu cầu hiệu nawg cao thường gom nhóm các yêu cầu đọc của nó vào và sắp xếp các yêu cầu đọc đó theo thứ tự rồi mới gửi lần lượt các yêu cầu đọc tới GFS, việc này giúp thao tác đọc dữ liệu của ứng dụng đó diễn ra theo hướng sequential read, qua đó giúp hiệu năng đọc dữ liệu qua GFS tăng lên.
- Bên cạnh 2 thao tác đọc trên, các ứng dụng client cũng trao đổi một khối lượng lớn dữ liệu với GFS thông qua thao tác nạp thêm dữ liệu vào file thông qua một lượng lớn các thao tác write-append. Điểm đặc biệt của thao tác write-append là thao tác này nạp dữ liệu vào cuối của file, chứ không phải là ghi đè dữ liệu vào một offset trước đó của file. Khối lượng dữ liệu mà mỗi thao tác write-append ghi thêm vào cuối file tương tự như thao tác streaming read (vài trăm KB - 1 MB). Thao tác ghi đè ngẫu nhiên vào một vị trí trong file được GFS hỗ trợ, tuy nhiên GFS sẽ không tối ưu hiệu năng cho thao tác này.
- Hệ thống cần xây dựng cơ chế cho phép một số lượng lớn các client có thể thực hiện thao tác write-append vào một file đồng thời (concurrently) với nhau. Trong hệ thống Google, sẽ có thể có hàng trăm client cùng 1 lúc thực hiện thao tác write-append vào một file trong GFS. Chúng ta cần đảm bảo các thao tác write-append này được đồng bộ hóa, cũng như giảm thiểu tối đa sự quá tải xảy ra khi thực hiện việc đồng bộ.
- Trong hệ thống sử dụng GFS, hệ thống mạng ưu tiên băng thông cao hơn độ trễ thấp, vì đa số các ứng dụng sử dụng GFS cần tốc độ xử lý - truyền tải dữ liệu cao (cần có băng thông cao) hơn thời gian phản hồi của GFS cho một yêu cầu đọc/ghi thấp (cần có độ trễ thấp).

### 2.2 Phương thức tương tác của các application với GFS

GFS cung cấp một giao diện tương tác thân thiện cho các ứng dụng sủ dụng nó, mặc dù giao diện này không được xây dựng theo một chuẩn API thông thường như POSIX. Các file trong hệ thống GFS được tổ chức theo thư mục phân cấp, và được định danh bằng đường dẫn tuyệt đối tới file đó. Các thao tác với File trong GFS là tạo mới file, xóa bỏ file, mở/đóng file và đọc/ghi file.
Hơn thế nữa, GFS cung cấp thêm 2 thao tác: tạo snapshot cho file và thao tác ghi-chèn file (record-append). Thao tác snapshow tạo ra một bản sao cho một file/thư mục trong GFS với chi phí nhỏ nhất. Thao tác ghi-chèn file cho phép  hàng loạt các client đồng thời chèn dữ liệu(append) vào một file cùng 1 lúc, trong khi vẫn đảm bảo sự đồng bộ của từng thao tác chèn dữ liệu của từng client. Thao tác record-append cho phép các ứng dụng client có thể ghi chèn vào cùng 1 file mà không cần tới cơ chế locking trong client. Thao tác snapshot và record-append sẽ được thảo luận thêm trong phần 3.4 và 3.3

### 2.3 Kiến trúc của GFS

Một GFS cluster được thiết kế bao gồm một master node và hàng loạt trunkserver được truy cập bởi nhiều client một lúc. Master node cũng như các trunkserver và các client được nhìn dưới góc nhìn là các Linux process,do đó chúng ta có thể có một máy vật lý đồng thời chạy cả trunkserver và client cùng một lúc, nếu chúng ta có thể chấp nhân rằng các máy vật lý này sẽ chia sẻ tài nguyên cho nhiều process, cũng như độ tin cậy giảm xuống.

Một file trong GFS được chia thành nhiều chunks. Mỗi một trunk trong GFS là một đơn vị lưu trữ, và được định danh bởi một ID có độ dài 64 bit có tính chất bất biến cũng như có tính toàn cục, tức là trong toàn bộ GFS cluster ID này chỉ đại diện duy nhất cho một chunk xác định. ID của chunk được gọi là **chunk handle**, nó được gán cho chunk tại thời điểm tạo ra chunk đó. Các chunkserver sẽ lưu trữ các chunk trong Local disk của máy chứa chunk server đó như là một file thông thường trong hệ điều hành Linux,  và thao tác đọc/ghi trên chunk được ánh xạ tới các thao tác đọc/ghi trên file. Đồng thời, các thao tác đọc/ghi trên GFS sẽ được xác định bởi 2 thành phần sau: **chunk handle** - xác định chunk nào sẽ được client tương tác, và byte range - xác đinh khoảng dữ liệu nào trên trunk sẽ được tương tác. Để đảm bảo độ tin cậy và khả năng chống lỗi, một chunk trên hệ thống sẽ được sao lưu thành 3 bản sao lưu trữ trên 3 chunkserver khác nhau. Bên cạnh đó, user cũng có thể thiết lập số lượng bản sao khác nhau cho từng file.

Master node duy trì tất cả mọi metadata của GFS cluster. Các metadata của GFS được lưu trữ trên Master node bao gồm namespace, thông tin về quyền truy cập tới một file của các client, ánh xạ giữa file - chunks (các chunk của một file có các chunkID là gì ?) và vị trí của các chunk trên hệ thống ( một chunk có _chunk handle_ là **x** nằm ở chunkserver nào trên hệ thống ?). Bên cạnh đó, master node quản lý các thao tác đồng bộ hóa và quản lý hệ thống như _chunk lease_, thao tác đồng bộ hóa và di chuyển dữ liệu giữa các chunk servers. Master node sẽ định kỳ truyền thông với từng chunkserver thông qua **HeartBeat** message để điều khiển các chunkserver cũng như thu thập trạng thái hiện tại của chunkserver.

GFS client application sẽ sử dụng các API để kết nối tới master node và các chunkserver để đọc/ghi dữ liệu. Client kết nối tới master node để truy cập vào metadata của các file, tuy nhiên tất cả các tương tác với dữ liệu của file lưu trữ trên GFS được Client thực hiện trực tiếp với Chunkserver (mà không cần đi qua master node).

Client và chunkserver đều không cache lại dữ liệu. Client cache không có nhiều tác dụng, lý do là vì kích thước các file mà client tương tác thường là rất lớn, đồng thời working set của các clien application lớn, không thể chứa vừa trong cache. Chunkserver không cache lại các file, do thực chất các chunk chính là các local file trên các chunkserver, do đó khi client tương tác với các file, hệ thống Linux của chunkserver đã tự động thực hiện việc tạo ra cache lưu trữ các file được thường xuyên truy cập trên bộ nhớ chính. Tuy nhiên, khi client tương tác với master node, metadata của các file trong GFS sẽ được cache lại client.

### 2.4 Master Node

Một GFS được thiết kế với chỉ một master node duy nhất. Lý do chỉ có 1 master node trong cluster, đó là để thiết kế hệ thống đơn giản, và cho phép chỉ có một node duy nhất chứa các thông tin toàn cục của hệ thống, để master node có thể thực hiện các công việc đồng bộ và sao lưu dữ liệu dễ dàng hơn. Tuy nhiên, với việc chỉ có một master node trong cluster, chúng ta phải tối ưu hóa hiệu suất của master node, để node này không trở thành bottleneck trên hệ thống. Client sẽ không đọc/ghi dữ liệu thông qua master node. Thay vào đó, client sẽ yêu cầu master node cung cấp metadata của file, qua đó biết được file/chunk mà nó muốn tương tác đang nằm ở chunkserver nào. Sau đó vị trí của chunk/file sẽ được client cache lại trong một khoảng thời gian nhỏ, đồng thời client sử dụng thông tin này để tương tác trực tiếp với trunserver trong các hoạt động tiếp theo của quá trình client đọc ghi dữ liệu trên GFS.

Quá trình client đọc dữ liệu từ một chunk trên một file được mô tả cụ thể như sau:

![Fig-1-GFS-architecture](./images/Fig-1-GFS-architecture.png)

Đầu tiên, do một chunk có kích thước xác định (64MB), Client sẽ tính toán ra tại 1 offset xác định trong file sẽ nằm trong chunk thứ mấy - chunk index của file(một file được ánh xạ thành một danh sách các chunk trên GFS). Sau đó, client gửi lên master node một request yêu cầu phân giải địa chỉ của chunk, với tham số đầu vào chứa bên trong request là tên file và chunkindex. Master node  phản hồi client với dữ liệu trả về là ChunkID - **x** của chunk mà người dùng muốn truy cập và chunk location - Địa chỉ của các chunkserver chứa các bản sao -replica của chunk có ChunkID là **x** này (vì như đã nói ở phần trước, một chunk trên hệ thống GFS được lưu trữ dưới dạng nhiều bản sao - replica trên các chunkserver khác nhai). Client cache lại thông tin địa chỉ của các replica này, và sử dụng chunkindex như là key để truy cập.

Sau đó, client lựa chọn một trong số các chunkserver chứa replica của x - thường là chunkserver gần với client nhất, rồi gửi tới chunkserver được chọn yêu cầu tương tác (đọc nội dung) với chunk x, với tham số truyền vào là **chunk handle** của x và **byte range** - Khoảng vị trí trong chunk mà Client muốn đọc (điều này có nghĩa là Client không muốn chunk server gửi về toàn bộ nội dung của chunk, mà chỉ gửi về một phần nội dung của chunk - xác định bởi **byte range**. Trong các request đọc tới chunk x tiếp theo. client có thể tiếp tục sử dụng thông tin của x đã được cache lại để tiếp tục truy cập cho tới khi cache bị hết hạn - expired hoặc file bị mở lại. Trong thực tế, client thường yêu cầu master node gửi lại địa chỉ của một loạt các chunk trong cùng 1 request.

### 2.5 Chunk size

Một trong những thông số quan trọng nhất cần xác định trong quá trình thiết kế GFS là kích thước của một chunk. Google chọn kích thước của một chunk là 64 MB, lớn hơn nhiều kích thước thông thường của một block - cluster trong hệ thống Linux File System. Như đã nói ở phần trước, trên một chunkserver, một chunk là một file trên hệ thống file Linux, và các chunk có kích thước không đủ 64MB (các chunk nằm ở cuối file) sẽ chỉ tăng kích thước khi Client nạp thêm ứng dụng vào file đó. Điều này cho phép hệ thống tránh được lãng phí, vì khi sử dụng kích thước cố định cho toàn bộ các chunk trong file, thì có một phần bộ nhớ trong file cuối cùng sẽ không chứa thông tin gì, dẫn tới lãng phí bộ nhớ của chunk server.

Chunk file có kích thước khá lớn, lý do là vì kích thước lớn của chunk cho phép một chunk chứa được nhiều dữ liệu, do đó giảm số lần tương tác giữa Client với master node. Ví dụ nếu chunk có kích thước là 16 MB, và các dữ liệu mà Client cần đọc nằm ở 4 chunk liên tiếp nhau, thì Client sẽ phải gửi lên master node request 4 yêu cầu phân giải địa chỉ của 4 chunk đó. Nhưng nếu như chúng ta sử dụng chunk có kích thước là 64MB, thì các dữ liệu mà Client muốn đọc sẽ nằm gọn trong 1 chunk, và Client sẽ chỉ cần gửi 1 yêu cầu phân giải tới master node. Tính chất giảm số lượng request phân giải khi kích thước chunksize lớn còn được cải thiện hơn nữa trong GFS, do đặc điểm đọc/ghi đữ liệu của các Client là thường đọc ghi một lượng lớn dữ liệu tuần tự, nên trong một khoảng dữ liệu gần nhau trong file, số lượng request yêu cầu tương tác là rất lớn. Một ưu điểm nữa của chunk có kích thước lớn, đó là khi Client chỉ phải tương tác dữ liệu với một chunk xác định trên một chunkserver, kết nối TCP từ Client tới chunkserver là cố định, qua đó làm giảm lượng tương tác cần thực hiện giữa Client và chunkserver cũng như làm giảm lượng kết nối cần thực hiện qua hệ thống mạng, nên hệ thống mạng được giảm tải. Ưu điểm thứ 3 của chunk kích thước lớn, đó là khi một chunk có kích thước lớn, thì số lượng chunk cần có cho một file trong GFS là nhỏ, nên số lượng metadata cần lưu trữ trên master node là nhỏ, vì mỗi một chunk cần có một metadata tương ứng để phân giải tên cho chunk đó. Khi metadata trên master node có kích thước nhỏ, chúng ta có thể lưu trữ trực tiếp lượng metadata này trên RAM, qua đó tăng hiệu năng của master node lên đáng kể.

Tuy nhiên, bên cạnh các ưu điểm trên, kích thước lớn của chunk cũng có một nhược điểm. Đó là do kích thước chunk lớn, do đó các file nhỏ sẽ có ít chunks, do đó nội dung của file này chỉ được phân bố trên một số lượng nhỏ chunkserver. Nếu các file này có nhu cầu truy cập lớn từ các Client, thì các chunkserver chứa các chunk của file này sẽ có khả năng bị quá tải. Tuy nhiên, nhược điểm này không phải là vấn đề quá nghiêm trọng, do đa số các Client thường làm việc với các file có kích thước lớn theo phương thức đọc tuần tự -sequentialy read.

Tuy nhiên, nhược điểm nêu trên vẫn có khả năng xảy ra đối với một số file đặc biệt, ví dụ như file của hệ thống batch-queue system. File này có kích thước nhỏ, chỉ được chứa trong một chunk, tuy nhiên lại được hàng trăm client sử dụng đồng thời, do đó các chunkserver chứa replica của file này có thể bị quá tải. Để giải quyết vấn đề này, chúng ta có thể tăng hệ số sao chép replica của file này lên, giúp cho nội dung của file này được lưu trữ trên nhiều chunkserver hơn. Một giải pháp khác, đó là cho phép một Client có thể đọc nội dung của file trên từ một Client khác chứ không thông qua chunkserver.

### 2.6 Metadata

Master node trên GFS cluster lưu trữ ba loại metadata sau: namespace của các file được lưu trữ trong GFS và của chunk, ánh xạ file - chunk và địa chỉ truy cập các replica của một chunk. Namespace của các file được lưu trữ trong GFS và của chunk và ánh xạ file - chunk giữ được tính chính xác nhờ vào một thành phần đặc biệt, đó là **operation log** - một file lưu lại các thay đổi đã xảy ra trên  file namespace, chunk namespace và ánh xạ file - chunk. Khi Client thực hiện một thay đổi nào đó trên 3 thành phần trên, một hành động **logging mutation** được sinh ra và thực hiện việc ghi nhận lại thay đổi lên **operation log**. **operation log** được lưu trữ trên master node và trên một số máy khác - được gọi là các **shadow master**, nhằm đề phòng trường hợp master node xảy ra sự cố. Sử dụng phương pháp log cho phép chúng ta cập nhật trạng thái master node đơn giản, tin cậy, hiệu quả và phòng tránh trường hợp thông tin lưu trữ trên master node mất tính nhất quán, tức là kể cả khi master node gặp sự cố thì thông tin của master node vẫn an toàn, do các thông tin này được backup được dự phòng trên các shadow master. Master node không lưu trữ thông tin về địa chỉ của các replica của một chunk. Thông tin về địa chỉ của các replica của một chunk được các chunkserver đẩy lên master node tại thời điểm master node bắt đầu khởi động, và tại thời điểm một chunkserver mới gia nhập vào GFS cluster.

#### 2.6.1 Cấu trúc của các thông tin lưu trữ trong RAM của master node

Như chúng ta đã trình bày, các thông tin nằm trên master node được lưu trữ trên bộ nhớ RAM để đảm bảo hiệu năng của master node, đồng thời điều này cho phép master node thực hiện thao tác kiểm tra định kỳ toàn bộ trạng thái của các thông tin này, cũng như trạng thái của bản thân master node một cách dễ dàng. Các công việc được master node thực hiện khi thao tác kiểm tra định kỳ được khởi động, đó là thực hiện việc thu gom các chunk nằm trong garbage, tạo ra các bản sao khác cho một chunk trên một chunkserver, trong trường hợp một chunkserver nào đó chứa một replica của chunk đó bị lỗi, và thực hiện việc di chuyển một số replica của một chunk từ chunkserver này sang chunkserver khác để cân bằng tải và cân bằng dữ liệu lưu trữ giữa các chunk server trên Cluster, trong trường hợp một số chunkserver trên cluster chứa quá nhiều chunk replica, trong khi đó một số chunkserver khác lại chứa quá ít chunkserver. Các thao tác vừa được nêu ra sẽ được thảo luận chi tiết tại các phần 4.3 và 4.4

Khi chúng ta sử dụng phương thức lưu trữ thông tin của master node trên RAM, một vấn đề quan trọng mà chúng ta cần lưu ý, đó là bộ nhớ RAM của master node có đủ lớn để lưu trữu tất cả các thông tin của các chunk hay không. Vấn đề kích thước bộ nhớ RAM của master node chính là lý do mà một chunk phải có kích thước đủ lớn. Trong thực tế, chúng ta cần sử dụng 64 byte để quản lý metadata của một chunk, do đó khi chunk có kích thước 64 MB, có nghĩa là chúng ta chỉ cần 64 byte trên master node để quản lý 64MB dữ liệu. Vì vậy, bộ nhớ RAM của master node vẫn đủ kích thước để chứa tất cả mọi thông tin cần lưu trữ trên master node, và trên thực tế, đây không phải là là vấn đề quá lớn trong GFS. Bên cạnh metadata của các chunk, thì một thông tin khác mà chúng ta cần lưu trữ, đó là file namespace. Tương tự chunk, kích thước bộ nhớ mà chúng ta cần sử dụng để lưu trữ file namespace là nhỏ, do chúng ta chỉ cần ít hơn 64 byte để lưu trữ thông tin của một file trong namespace, vì trong hệ thống GFS, chúng ta có thể nén tiền tố của một file.

Trong trường hợp hệ thống GFS cần mở rộng, thì chi phí cần thiết để mở rộng bộ nhớ RAM của master node vẫn là rất nhỏ so với các lợi ích mang lại khi chúng ta sử dụng RAM để lưu trữ thông tin metadata trên master node.

#### 2.6.2 Vị trí của chunk

Master node không lưu trữ cố định thông tin về vị trí của các chunk - thông tin chỉ ra các chunkserver nào đang lưu trữ bản sao của một chunk có ID là **x**. Thay vào đó, thông tin này sẽ được các chunkserver gửi lên master node trong lúc các node trên cluster khởi động. Sau đó, trong quá trình cluster hoạt động, master node có thể tự động cập nhật thông tin về vị trí của các chunk, vì master node chính là node điều khiển hoạt động quản lý các chunk trên cluster như xác định các replica của một chunk của một file mới tạo sẽ nằm trên các chunkserver nào, theo dõi trạng thái của các chunkserver... Master node quản lý các chunk thông qua việc tương tác với các chunkserver bằng **HeartBeat** message.

Vào thời điểm bắt đầu thiết kế GFS, thông tin về vị trí của chunk đã từng được thử đặt cố định trên master node. Tuy nhiên, trong quá trình thử nghiệm sau đó, chúng ta nhận ra thông tin về vị trí của chunk có thể được duy trì và cập nhật dễ dàng hơn trên master node thông qua phương thức sau: Master node khởi tạo thông tin về vị trí của chunk bằng cách lấy thông tin này từ các chunkserver, sau đó trong quá trình cluster hoạt động, master node định kỳ gửi request xuống các chunkserver để cập nhật vị trí của các chunk. Phương thức này cho phép xử lý vấn đề đồng bộ thông tin giữa master và các chunkserver hiệu quả hơn phương pháp đặt thông tin về vị trí của chunk cố định trên master node, trong các trường hợp chunkserver gia nhập hoặc rời khỏi cluster, tên folder/file thay đổi, chunkserver gặp lỗi hoặc khởi động lại,... Trong một cluster có hàng trăm server, các sự kiện này thường xuyên xảy ra.

Một lý do khác để Google quyết định sử dụng phương thức đã trình bày để lưu trữ và cập nhật thông tin về vị trí của các chunk trong GFS, đó là chỉ có chunkserver mới có thể biết rõ nhất chunkserver đó đang giữ các chunk nào trong hệ thống. Thông tin về vị trí của một chunk trên hệ thống GFS không có tính chất cố định, bất biến, do các sự cố trên hệ thống xảy ra có thể làm thay đổi thông tin này bất kỳ lúc nào. Ví dụ, như tại một thời điểm xác định, chúng ta có thể có được thông tin chunk **k** có replica nằm trên các server **c1**, **c2**, **c3**. Nhưng sau đó một thời gian, chunkserver **c2** bị hỏng 1 ổ cứng, và ổ cứng đó chứa replica của **k**, lúc này thông tin về vị trí của **k** bị thay đổi, trở thành: **k** có 2 replica nằm trên các server **c1** và **c3**. Sự thay đổi thường xuyên của thông tin này chính là lý do mà việc lưu trữ cố định thông tin này trên master node là không hiệu quả trên thực tế, vì khi sự cố xảy ra trên chunkserver sẽ dẫn tới việc các client sẽ không kết nối được tới các chunkserver, có nghĩa là hệ thống mất đi tính ổn định, chống lỗi và chính xác.

#### 2.6.3 **Operation log**

Operation log là thành phần lưu trữ các bản ghi lịch sử ghi lại các thay đổi của metadata trên cluster. Operation log là một trong các thành phần trung tâm trong GFS. Thành phần này không chỉ là nơi chứa metadata của GFS, mà còn là thành phần điều khiển việc đồng thời thực thi một loạt các thao tác (concurrent operations) trong hệ thống, bằng cách xác định thứ tự thực hiện các thao tác này và ghi lại các thao tác này cũng như thứ tự thực hiện chúng vào operation log. Các file, chunk và các version của chúng (xem section 4.5), có tính duy nhất trong toàn bộ Cluster, tính duy nhất này được xác định bằng thời điểm logic mà chúng được tạo ra.

Vì operation log có tính quan trọng và mang tính quyết định tới các hoạt động trong GFS cluster, do đó chúng ta phải duy trì tính tinh cậy của operation log bằng cách không cho phép người dùng nhìn thấy được sự thay đổi của metadata cho tới khi các thay đổi trên metadata của hệ thống được thực hiện xong và metadata trên hệ thống trở lại ổn định. Bên cạnh đó,  chúng ta có nguy cơ mất toàn bọ file system hoặc làm mất một số thao tác từ client trong trường hợp master node gặp sự cố, do đó chúng ta cần sao lưu operation log sang các máy khác (remote machine). Vì vậy, hệ thống chỉ phản hồi cho client biết được một thao tác mà client vừa thực hiện có thành công hay không sau khi mọi thay đổi trên metadata (tạo ra khi thao tác của của client được thực thi) được lưu lại trên master node cũng như trên các máy giữ bản sao của metadata.

Trong trường hợp master node gặp sự cố, nó có thể phục hồi lại file system của hệ thống bằng cách duyệt lại operation log. Để tổi thiểu hóa thời gian duyệt lại operation log, chúng ta cần đảm bảo từng log được lưu trong operation log với kích thước nhỏ.  Master node sẽ tạo ra checkpoint trên operation log bất cứ khi nào operation log mở rộng tới một kích thước xác định. Do vậy, khi master node cần phục hồi lại metadata, nó có thể tải lại checkpoint cuối cùng mà nó tạo ra trong operation log và master node sẽ chỉ cần replay lại các log được lưu lại sau checkpoint đó lên checkpoint (vì checkpoint đã là một bản sao của các metadata tại thời điểm đó được tạo ra). Việc replay lại các thay đổi lưu trên log được hiểu như là việc áp dụng các thay đổi đó lên checkpoint mới nhất để tạo ra trạng thái hiện tại của hệ thống. Checkpoint trong operation log được tạo ra dưới dạng một B-tree, đo đó chúng ta có thể ánh xạ trực tiếp checkpoint này vào bộ nhớ và sử dụng nó để tìm kiếm các tên riêng (của file/chunk) trong các namespace mà không cần làm thêm các thao tác phân tích (parsing). Tính chất này giúp cho GFS có thể thực hiện quá trình phục hồi hệ thống nhanh chóng, đồng thời cải thiện tính sẵn sàng của hệ thống.

Vì thao tác xây dựng một check point có thể tốn một thời gian khá lớn, do đó master node cần có phương thức xây dựng checkpoint sao cho quá trình xây dựng một checkpoint không làm ảnh hưởng tới hoạt động của hệ thống, tức là trong khi một checkpoint mới được xây dựng, thì hệ thống vẫn có thể tiếp nhận và thực hiện các thay đổi lên metadata. Để làm được điều này, phương thức được sử dụng trong GFS, đó là chúng ta sẽ copy các log file và tạo checkpoint trong một tiến trình độc lập. Checkpoint được tạo ra sẽ chứa tất cả các thay đổi trước thời điểm nó được tạo ra. Một checkpoint trong một cluster có khoảng vài triệu file sẽ được tạo ra trong khoảng một phút. Sau khi checkpoint mới được tạo ra, nó sẽ được ghi lên đĩa của master node cũng như copy sang các máy remote.

Thao tác phục hồi metadata của hệ thống chỉ cần 2 thông tin để thực hiện, đó là checkpoint cuối cùng được ghi lại và các log file được tạo ra sau checkpoint đó. Các checkpoint và log file cũ hơn checkpoint cuối cùng hoàn toàn có thể được xóa bỏ khỏi hệ thống, mặc dù chúng ta có thể giữ lại một số thành phần để đảm bảo tính toàn vẹn của hệ thống. Nếu quá trình checkpoint gặp sự cố, quá trình phục hồi của hệ thống vẫn có thể được thực hiện, vì thao tác recovery có thể phát hiện ra checkpoint bị lỗi và loại bỏ nó và sử dụng một checkpoint cũ hơn để thực hiện quá trình phục hồi.

### 2.7 Mô hình consistency của GFS

Như các hệ thống phân tán khác, GFS cũng được xây dựng một mô hình consistency để đảm bảo tính ổn định của hệ thống phân tán. Mô hình consistency của GFS có tính đơn giản và hiệu quả trong hệ thống GFS có mức độ phân tán cao. Trong phần này, chúng ta sẽ thảo luận cách mà GFS đảm bảo độ tin cậy của dữ liệu trong hệ thống, và ý nghĩa của sự tin cậy của dữ liệu đối với các ứng dụng trong GFS. Bên cạnh đó, chúng ta cũng sẽ nói về cách GFS duy trì sự tin cậy, tuy nhiên chi tiết về cách mà GFS thực hiện việc này sẽ được nói tới ở các phần sau của bài viết.

Trước hết, chúng ta sẽ làm rõ về mô hình consistency mà GFS sử dụng, mô hình **relax consistency**.

Mô hình nhất quán (consistency model) trong một hệ thống phân tán là tập hợp các cơ chế, phương thức được sử dụng để duy trì tính nhất quán của hệ thống phân tán đó. Một hệ thống được gọi là có tính nhất quán, nếu như dữ liệu/thông tin trong các bộ nhớ (bao gồm cả bộ nhớ trong - RAM và bộ nhớ ngoài - Hard Disk) của hệ thống phân tán đó được xử lý theo một quy luật xác định. Mô hình nhất quán xác lập một thỏa thuận giữa hệ thống và những người lập trình ứng dụng Client, trong đó hệ thống đảm bảo cho Client sử dụng hệ thống rằng, nếu Client tuân thủ các nguyên tắc xử lý dữ liệu trong thỏa thuận, thì dữ liệu lưu trữ trong hệ thống sẽ có tính nhất quán, và Client có thể xác định trước được kết quả đầu ra của quá trình xử lý một dữ liệu/thông tin trong hệ thống.

Consistency model mà GFS sử dụng là relax consistency. Mô hình này được xây dựng dựa trên định lý sau về tính consistency của hệ thống phân tán: Định lý CAP của Eric Brewer

Định lý CAP được phát biểu như sau:

#### 2.7.1 Cách thức đảm bảo sự tin cậy của GFS

Các thay đổi trong không gian tên của GFS cluster (ví dụ hành động tạo một file mới) có tính nguyên tử. Những thay đổi này được xử lý bởi master theo phương thức sau: Các sự thay đổi trong không gian tên sẽ cần tới các khóa (locking) để đảm bảo sự tin cậy, các khóa này được tạo ra trong không gian tên ở mức độ đơn vị (sẽ nói rõ hơn ở phần 4.1), đổng thời để đảm bảo tính đúng đắn, operation log trên master node sẽ định nghĩa và lưu lại thứ tự của các thay đổi được thực hiện ở phạm vi toàn cục (global) trên cluster.

Trạng thái của một file trong GFS sau một thao tác thay đổi dữ liệu phục thuộc vào các yếu tố như loại thay đổi được thực hiện, việc thực hiện thay đổi có thành công hay không, và tại thời điểm thực hiện thay đổi có bao nhiêu thao tác thay đổi được thực hiện đồng thời trên file này. Bảng 1 Tóm tắt các khả năng có thể xảy ra đối với trạng thái của một file:

